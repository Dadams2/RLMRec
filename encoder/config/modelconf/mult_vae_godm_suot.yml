optimizer:
  name: adam
  lr: 1.0e-3
  weight_decay: 0

train:
  epoch: 1000
  batch_size: 1024
  save_model: false
  loss: pairwise
  test_step: 3
  reproducible: true
  seed: 2024
  patience: 20
  trainer: vae_trainer

test:
  metrics: [recall, ndcg]
  k: [5, 10, 20]
  batch_size: 1024

data:
  type: general_cf
  name: yelp


model:
  name: mult_vae_godm_suot
  # general parameters here

  # Sliced Unbalanced Optimal Transport specific parameters
  num_projections: 50      # Number of random projections for slicing
  rho1: 10.0               # Marginal relaxation for source distribution (collaborative)
  rho2: 10.0               # Marginal relaxation for target distribution (LLM)
  suot_niter: 10           # Number of Frank-Wolfe iterations
  suot_p: 2                # Power for cost function (2 = Wasserstein-2)
  suot_mode: icdf          # 'icdf' (default, stable) or 'backprop' for computing potentials
  
  # dataset-specific parameters here
  dropout: 0.2
  reg_weight: 1.0e-6
  
  # for amazon
  amazon:
    dropout: 0.3
    beta: 0.3              # Lower beta might work with SUOT due to better gradient flow
    reg_weight: 1.0e-6
    num_projections: 50
    rho1: 10.0
    rho2: 10.0
    suot_niter: 10
    suot_mode: icdf
    
  # for yelp
  yelp:
    dropout: 0.3
    beta: 0.6              # SUOT may allow lower beta than standard GODM (0.8)
    reg_weight: 1.0e-6
    num_projections: 50
    rho1: 10.0
    rho2: 10.0
    suot_niter: 10
    suot_mode: icdf
    
  # for steam
  steam:
    dropout: 0.2
    beta: 0.8              # SUOT may allow lower beta than standard GODM (1.0)
    reg_weight: 1.0e-6
    num_projections: 50
    rho1: 10.0
    rho2: 10.0
    suot_niter: 10
    suot_mode: icdf

  embedding_size: 32